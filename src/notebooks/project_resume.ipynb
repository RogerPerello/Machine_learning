{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "- Se tiene como objetivo inicial de este proyecto superar la cifra de la predicción ganadora de [una competición de Kaggle ya cerrada](https://www.kaggle.com/competitions/diamonds-part-datamad0122/)\n",
    "\n",
    "- El objetivo secundario es, mediante un segundo modelo que saque las características de un diamante a partir de su foto, obtener los datos necesarios para predecir el precio. [El \"dataset\" con las imágenes de los diamantes y sus características se obtiene, también, de Kaggle](https://www.kaggle.com/datasets/harshitlakhani/natural-diamonds-prices-images)\n",
    "\n",
    "- Se usa un tercer modelo en lugar del de la competición para obtener los precios a partir de esas características. Ese otro modelo se entrena con [el \"dataset\" original de diamantes](https://www.kaggle.com/datasets/swatikhedekar/price-prediction-of-diamond), pues su variable \"target\" es más precisa que la del torneo, dado que no se ha escalado ni redondeado\n",
    "\n",
    "- Con el modelo que obtiene las características de los diamantes a partir de una foto y el modelo que predice los precios se crea, además, [una \"app\" de Streamlit que les saca partido](https://rogerperello-machine-learning-diamon--streamlitfilesmain-bke5k8.streamlit.app/)\n",
    "\n",
    "- La fuente del \"dataframe\" de competición y el original, según señala el autor en Kaggle, es esta: \"[Tiffany & Co's snapshot pricelist from 2017](https://www.chegg.com/homework-help/questions-and-answers/data-source-tiffany-co-s-snapshot-price-list-2017-data-detail-ofiginal-dataset-peblishod-t-q103429046)\" \n",
    "\n",
    "- En cuanto al autor de las imágenes de los diamantes, también en Kaggle, [las ha obtenido con \"webscrapping\"](https://capitalwholesalediamonds.com/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "- Es el análisis exploratorio de los datos de competición. Como las variables son las mismas, los cambios detectados posibles se extrapolan para el \"dataset\" original\n",
    "\n",
    "- Se lleva a cabo en el \"notebook\" homónimo\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones iniciales\n",
    "\n",
    "- Se adaptan los nombres de las columnas para que sean legibles\n",
    "\n",
    "- No se detectan nulos\n",
    "\n",
    "- Se encuentran duplicados, pero como todos tienen su propia identificación se asume que cada diamante es único. Por tanto, se conservan\n",
    "\n",
    "- Se detecta que hay diamantes con 0 milimetros de altura, anchura y/o profundidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambios probados\n",
    "\n",
    "- Se detectan una serie de características:\n",
    "\n",
    "1) Varias columnas tienen unos pocos valores atípicos extremadamente altos [\"depth (percentage)\", \"table (percentage)\", \"width (millimeters)\", \"depth (millimeters)\"]\n",
    "\n",
    "2) Como se ha anticipado, varias filas tienen un cero en todas las variables relacionadas con el tamaño de los diamantes, sin contar el peso [\"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "3) Dos columnas con una alta correlación negativa tienen unos pocos \"outliers\" que destacan moderadamente y que, además, son compartidos [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "4) Si se borrasen las filas con ceros del punto 2, todavía quedaría un cero en una fila para el \"length\". Como los diamantes son prácticamente circulares, \"length\" y \"width\" son casi idénticas [\"length (millimeters)\"].\n",
    "\n",
    "5) Si se borrasen las filas con ceros del punto 2 y se solucionase el cero del punto 4, todavía quedaría un cero en una fila para \"depth (millimeters)\". Se descubre que la variable \"depth (percentage)\" se obtiene de dividir el \"depth (millimeters)\" correspondiente entre la media de \"width\" y \"length\" [\"depth (millimeters)\"].\n",
    "\n",
    "6) Se detecta un \"outlier\" en \"lenght\" [\"length (millimeters)\"].\n",
    "\n",
    "7) Varias columnas son asimétricas por la derecha. Con el logaritmo, se centrarían esas distribuciones y deaparecerían \"outliers\" [\"weight (carat)\", \"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "8) Existe un valor atípico en \"weight\". Sin embargo, imputarlo al valor máximo de su \"boxplot\" haría que resurgiese después de un escalado estándar [\"weight (carat)\"].\n",
    "\n",
    "9) Dos columnas tienen una alta correlación negativa que se quiere conservar, y muchos \"outliers\" no compartidos. Se presume que la posición de esos \"outliers\" es la que determina la correlación [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "10) \"depth (millimeters)\" tiene valores atípicos. Esta variable, junto con las demás relacionadas con el tamaño, están tan correlacionadas que casi podría considerarse que son casi la misma variable escalada [\"depth (millimeters)\"].\n",
    "\n",
    "11) Algunas variables tienen valores mucho más altos que el resto.\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "- Con esa información, se prueban una serie de posibles cambios a llevar a cabo durante la fase de \"feature_engineering\". Si se aplica el conjunto de estos cambios, el \"dataframe\" queda libre de nulos completamente:\n",
    "\n",
    "1) Borrado de \"outliers\" extremadamente altos [\"depth (percentage)\", \"table (percentage)\", \"width (millimeters)\", \"depth (millimeters)\"].\n",
    "\n",
    "2) Borrado de filas que tienen 0 en todas las variables relacionadas con el tamaño excepto el peso [\"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "3) Borrado de los \"outliers\" compartidos moderadamente altos [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "4) Asignación del valor con 0 restante en \"length\" al \"width\" correspondiente [\"length (millimeters)\"].\n",
    "\n",
    "5) Asignación del valor con 0 restante de \"depth (millimeters)\" a partir de una operación con el \"length\", el \"width\" y el \"depth (percentage)\" correspondientes [\"depth (millimeters)\"].\n",
    "\n",
    "6) Asignación del \"outlier\" restante del \"length\" al \"width\" correspondiente [\"length (millimeters)\"].\n",
    "\n",
    "7) Transformación con logaritmo [\"weight (carat)\", \"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "8) Imputación al siguiente valor más alto [\"weight (carat)\"].\n",
    "\n",
    "9) Imputación a los valores máximos y mínimos del \"boxplot\" [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "10) Neutralización de \"outliers\" con un modelo \"ridge\" [\"depth (millimeters)\"].\n",
    "\n",
    "11) Escalado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambios no probados\n",
    "\n",
    "- Se detectan una serie de características:\n",
    "\n",
    "12) La variable \"depth (percentage)\" sale de dividir \"depth (millimeters)\" por la media de \"length (millimeters)\" y \"width (millimeters)\". Por tanto, si se hace la operación, el resultado debería coincidir con la columna de \"depth (percentage)\". Sin embargo, no es así. Habrá que probar durante el \"feature engineering\" si utilizar los valores calculados mejora la predicción.\n",
    "\n",
    "13) Las columnas relacionadas con el tamaño tienen una correlación altísima. Quizá los resultados mejoren con borrarlas. Otras columnas tinen una correlación ínfima, cercana a 0, que podría considerarse irrelevante\n",
    "\n",
    "14) Las filas con el valor máximo de \"clarity quality\" (7, variable discreta) no cambian en cuanto a las variables relacionadas con el tamaño si se comparan con las que tienen una \"clarity quality\" un punto por debajo (6). Quizá cabría imputar esos valores 7 de \"clarity quality\" al 6.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature_engineering\n",
    "\n",
    "- Se aplican los cambios probados y no probados durante el EDA intercalando múltiples modelos para ver 1) qué modelos mejoran con qué cambios, y 2) qué modelo es el adecuado para la optimización. Se trabaja con el \"dataset\" de competición, pero los resultados son extrapolables al original\n",
    "\n",
    "- Se lleva a cabo en el \"notebook\" homónimo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de cambios\n",
    "\n",
    "- Se reúnen los cambios apuntados en el EDA en subgrupos:\n",
    "\n",
    "a) Borrado: 1, 2 y 3 (borrado parcial: solo 2)\n",
    "\n",
    "b) Asignación: 4, 5, 6 (asignación parcial: solo 4 y 5)\n",
    "\n",
    "c) Logaritmo: 7\n",
    "\n",
    "d) Valor más alto de \"weight\": 8\n",
    "\n",
    "e) Imputaciones \"boxplot\": 9\n",
    "\n",
    "f) Imputaciones \"ridge\": 10\n",
    "\n",
    "g) Escalado \"MinMax\": 11\n",
    "\n",
    "h) Escalad \"Standard\": 11\n",
    "\n",
    "i) Sustitución: 12\n",
    "\n",
    "j) Descarte correlación altísima: 13 -> ningún modelo mejora\n",
    "\n",
    "j) Descarte correlación ínfima: 13\n",
    "\n",
    "k) Imputaciones \"clarity quality\": 14\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "- Cada uno de los modelos mejora con los siguientes cambios:\n",
    "\n",
    "\n",
    "---------- LinearRegression ----------\n",
    "\n",
    "· Borrado\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Logaritmo\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Imputaciones \"clarity quality\"\n",
    "\n",
    "---------- Ridge ----------\n",
    "\n",
    "· Escalado \"MinMax\"\n",
    "\n",
    "· Borrado\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Logaritmo\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Imputaciones \"clarity quality\"\n",
    "\n",
    "---------- KNeighborsRegressor ----------\n",
    "\n",
    "· Escalado \"Standard\"\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Valor más alto de \"weight\"\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Descarte correlación ínfima\n",
    "\n",
    "· Imputaciones \"clarity quality\"\n",
    "\n",
    "---------- SVR ----------\n",
    "\n",
    "· Escalado \"Standard\"\n",
    "\n",
    "· Borrado\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Logaritmo\n",
    "\n",
    "· Valor más alto de \"weight\"\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Descarte correlación ínfima\n",
    "\n",
    "---------- DecisionTree ----------\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación parcial\n",
    "\n",
    "---------- RandomForest ----------\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación parcial\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "---------- XGBRegressor ----------\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación parcial\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelos\n",
    "\n",
    "- Estos son los resultados, una vez aplicados los cambios y tuneados algunos hiperparámetros, si se compara la predicción con el \"y_test\":\n",
    "\n",
    "a) \"LinearRegression\" mejora su \"rmse\" de 0.182664 a 0.143851 (-21.24%)\n",
    "\n",
    "b) \"Ridge\" mejora su \"rmse\" de 0.182826 a 0.144234 (-21.11%)\n",
    "\n",
    "c) k vecinos mejora su \"rmse\" de 0.181632 a 0.121120 (-33.32%)\n",
    "\n",
    "d) \"SVR\" mejora su \"rmse\" de 0.189328 a 0.100319 (-47.01%)\n",
    "\n",
    "e) \"DecisionTree\" mejora su \"rmse\" de 0.130480 a 0.109705 (-15.92%)\n",
    "\n",
    "f) \"RandomForest\" mejora su \"rmse\" de 0.094246 a 0.091972 (-2.41%)\n",
    "\n",
    "g) \"XGBRegressor\" mejora su \"rmse\" de 0.090762 a 0.086893 (-4.26%)\n",
    "\n",
    "- Se elige trabajar en la siguiente fase con \"XGB\". Es el mejor con una \"rmse\" de 0.086893, y, como dispone de muchos más hiperparámetros que los probados hasta ahora, probablemente tenga un margen considerable de optimización\n",
    "\n",
    "- El orden, si \"XGB\" (1) no cumpliese o fuera necesario otro modelo, sería: 2) \"RandomForest\"; 3) \"SVR\"; 4) \"DecisionTree\"; 5) k vecinos; 6) regresión lineal; y 7) \"ridge\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target_transformation\n",
    "\n",
    "- Se confirma que la variable \"target\" del \"dataframe\" de competición se ha escalado, y no representa los precios reales de los diamantes\n",
    "\n",
    "- En el \"notebook\" homónimo se investiga si es cierto que los dos \"datasets\" son la misma cosa y qué tipo de escalado se ha usado\n",
    "\n",
    "- El escalado efectuado es el logaritmo. Sin embargo, se han perdido decimales en el proceso. Por tanto, hay que usar el \"dataframe\" de competición para competir, y el original para predecir precios reales con mayor concreción\n",
    "\n",
    "- Se exporta el original procesado sin la columna \"table\", pues esa columna tampoco existe en el \"dataframe\" sobre las características de las fotos de diamantes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images_data_processing\n",
    "\n",
    "- En el \"notebook\" homónimo, se tratan varios \"datasets\" a parte que representan un conjunto de características de imágenes de diamantes. \n",
    "\n",
    "- El proceso es el siguiente:\n",
    "\n",
    "1) Se unen los \"datasets\" en un solo \"dataframe\"\n",
    "\n",
    "2) Mediante diversas técnicas, se adapta el \"dataframe\" resultante para que contenga las mismas variables que se pretende pasar al otro modelo para hacer su predicción de los precios. No se puede disponer de la variable \"table\", sin embargo, así que el modelo que se use para predecir los precios a partir de las características obtenidas de las imágenes debe entrenarse sin esa variable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_optimization_price\n",
    "\n",
    "- Donde se experimenta de forma extensiva con los modelos necesarios con tal de obtener los mejores resultados posibles\n",
    "\n",
    "- El primer modelo entrenado (\"XGB\") que sale es el que se usa para la competición.\n",
    "\n",
    "- El segundo (también \"XGB\") y el tercero (k vecinos), entrenados con el \"dataframe\" original de diamantes, no el de competición, y sin la columna \"table\", se usan para predecir los precios reales de los diamantes de las fotos. Se crean dos porque \"XGB\" por si solo es incapaz de dar resultados correctos si se le pasan, para la predicción, valores por encima o por debajo de los máximos o mínimos de las variables vistas durante el entrenamiento. Eso es un problema recurrente con los modelos de árboles, y \"SVR\" es muy lento como alternativa, así que se elige k vecinos.\n",
    "\n",
    "- Luego, se hace un \"stacking\" de \"XGB\" y k vecinos para obtener un cuarto modelo, el bueno, para predecir los precios en la \"app\". Ese modelo combina la solidez de \"XGB\" y la flexibilidad de k vecinos para predecir con valores por encima o por debajo de los máximos y mínimos de las variables vistas durante el entrenamiento.\n",
    "\n",
    "- El proceso se lleva a cabo en el \"notebook\" homónimo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras para competición\n",
    "\n",
    "- Estos son los hiperparámetros que se añaden al modelo a cada ronda:\n",
    "\n",
    "1) Se prueba sin modificación alguna para tener una base desde la que hacer comparaciones\n",
    "\n",
    "2) Se prueba con los cambios de \"feature engineering\" (borrado parcial y asignación parcial) por la misma razón\n",
    "\n",
    "3) Se obtiene un número de estimadores óptimo como punto de partida y se tunea el \"learning rate\" (\"eta\"). Una vez tuneado, se vuelve a buscar el número óptimo de estimadores\n",
    "\n",
    "4) Se aplican \"constaints\" a la columna \"weight\", ya que solo crece con el precio\n",
    "\n",
    "5) Se establecen un \"subsample\" y \"colsample_bytree\" de 0.8, que es el estándar recomendable, y se tunean \"max_depth\" y \"min_child_weight\"\n",
    "\n",
    "6) Se tunea \"gamma\", parámetro de semiregularización\n",
    "\n",
    "7) Se prueban dos opciones y se tunean: a) \"sampling\" combinado con \"colsample_bytree\", y b) \"sampling_method='gradient_based\" combinado con \"colsample_bytree\". Gana la opción \"a\"\n",
    "\n",
    "8) Se tunean los hiperparámetros de regularización (\"alpha\", \"lambda\" y \"max_delta_step\"), que son los que ayudan a prevenir el \"overfitting\" y el \"underfitting\"\n",
    "\n",
    "9) Se aumenta el número de árboles en paralelo, convirtiendo, a todos los efectos, el \"xgboost\" en un \"random forest\" con \"booster\"\n",
    "\n",
    "10) Se reajusta el número de estimadores una vez más"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado competición\n",
    "\n",
    "- Se superan los puntos del ganador de la competición, con una \"rmse\" de 0.08506 contra 0.08617\n",
    "\n",
    "- Como el modelo predice bien con datos que no ha visto, hace pensar que no existe \"overfitting\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras para visualización A\n",
    "\n",
    "- Se repite el mismo proceso que en el punto anterior, pero esta vez se le quita la columna \"table\" al \"dataframe\", y se usa el original en lugar del de competición\n",
    "\n",
    "- Esto se hace porque en el \"dataframe\" que almacena los datos sobre las fotos de los diamantes no existe esa columna, como se ha visto en el \"notebook\" llamado \"images_data_processing\". Por tanto, es una variable que no se puede obtener a partir de una foto. Ello obliga a disponer de un modelo a parte del de la competición para la \"app\". Se usa el \"dataset\" original porque los precios son más precisos. Se le aplica el logaritmo a la \"target\" para facilitar el proceso, pero se conservan todos los decimales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado A\n",
    "\n",
    "-  Es mejor que el del anterior modelo (normal, ya que se han usado los precios reales)\n",
    "\n",
    "- Al probar con pasarle diferentes combinaciones de valores, se detecta que este modelo no predice bien cuando recibe cifras que superan o que son inferiores a los máximos y mínimos de las variables vistas durante el entrenamiento. Eso ocurre porque los modelos de árboles no son tan precisos en los extremos, y, como se ha notado en \"feature_enginerring\" y en el EDA, la \"r2\" es alta, y hay varias variables con una correlación altísima. Por eso, se crea un modelo B de predicción de precios\n",
    "\n",
    "- Aunque pudiera parecerlo, eso no ocurre porque haya \"overfitting\", pues el modelo \"xgboost\" usado para la competición ha predicho a partir de datos desconocidos estupendamente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras para visualización B\n",
    "\n",
    "- Con tal de predecir el precio de diamantes cuyas características sean superiores o inferiores a los máximos o mínimos de las variables vistas durante el entrenamiento, se elige el siguiente mejor modelo no \"de árboles\" detectado durante en \"feature_engineering\": \"SVR\"\n",
    "\n",
    "- Sin embargo, como su coste computacional es tan elevado, y se presume que será a aún mayor si se lleva a cabo un \"stacking\", se da preferencia el siguiente de la lista: k vecinos\n",
    "\n",
    "- A pesar de lo visto en el \"feature_engineering\", como se pretende aprovechar el modelo para valores muy altos o bajos a la hora de predecir, interesa entrenarlo con \"outliers\". Por tanto, no se le hace más tratamiento que el que ha recibido \"xgboost\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado B\n",
    "\n",
    "- La predicción con el \"y_test\" es peor, lo que confirma que \"xgboost\" trabaja mejor que k vecinos. Sin embargo, es un buen complemento para predecir si los valores que se le pasan están fuera de los máximos y mínimos de las variables vistas en el entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Stacking\"\n",
    "\n",
    "- Se combinan las bondades de ambos modelos con un \"stacking\". Para ello se usa un tercer modelo sencillo de regresión lineal\n",
    "\n",
    "- El modelo resultante predice tan bien como \"xboost\" y, además, es capaz de predecir a partir de valores superiores o inferiores a los máximos y mínimos de las variables vistas durante el entrenamiento\n",
    "\n",
    "- Así, este es el modelo que se usa para la predicción de precios en la \"app\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation_analysis\n",
    "\n",
    "- [Se utiliza \"webscrapping\"](https://www.in2013dollars.com/Jewelry/price-inflation/2017-to-2023?amount=326) para obtener automáticamente la cifra del aumento por inflación respecto a 2017, fecha en que se compuso el \"dataframe\" original de precios\n",
    "\n",
    "- En 2023, respecto a 2017, los precios han subido cerca de un 11%\n",
    "\n",
    "- Se implementa este método en la \"app\"\n",
    "\n",
    "- Se detecta también una alternativa para aproximar la inflación mediante la media de los últimos años, en caso de que la página dejara de funcionar puntualmente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images_processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ff123141b49d3e269b3661a34ce4932beef2f3780d5673d84a155db1cd61f7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
