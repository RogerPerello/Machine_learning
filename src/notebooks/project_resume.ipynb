{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "- Se tiene como objetivo inicial de este proyecto superar la cifra de la predicción ganadora de [una competición de Kaggle ya cerrada](https://www.kaggle.com/competitions/diamonds-part-datamad0122/) y ser capaz de predecir el precio de cualquier diamante a partir de sus características\n",
    "\n",
    "- Se usa un segundo en lugar del de la competición para obtener los precios a partir de esas características. Ese otro modelo se entrena con [el \"dataset\" original de diamantes](https://www.kaggle.com/datasets/swatikhedekar/price-prediction-of-diamond), pues su variable \"target\" es más precisa que la del torneo, dado que no se ha escalado ni redondeado\n",
    "\n",
    "- El objetivo secundario es predecir el precio a partir de una imagen. Esto se hace con una combinación de modelos. [El \"dataset\" con las imágenes de los diamantes y sus características se obtiene, también, de Kaggle](https://www.kaggle.com/datasets/harshitlakhani/natural-diamonds-prices-images)\n",
    "\n",
    "- Con los modelos que obtienen el precio a partir de una foto y el que predice los precios a partir de sus características se crea, además, [una \"app\" de Streamlit que les saca partido](https://rogerperello-machine-learning-diamon--streamlitfilesmain-bke5k8.streamlit.app/)\n",
    "\n",
    "- La fuente del \"dataframe\" de competición y el original, según señala el autor en Kaggle, es esta: \"[Tiffany & Co's snapshot pricelist from 2017](https://www.chegg.com/homework-help/questions-and-answers/data-source-tiffany-co-s-snapshot-price-list-2017-data-detail-ofiginal-dataset-peblishod-t-q103429046)\" \n",
    "\n",
    "- En cuanto al autor de las imágenes de los diamantes, también en Kaggle, [las ha obtenido con \"webscrapping\"](https://capitalwholesalediamonds.com/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "- Es el análisis exploratorio de los datos de competición. Como las variables son las mismas, los cambios detectados posibles se extrapolan para el \"dataset\" original\n",
    "\n",
    "- Se lleva a cabo en el \"notebook\" homónimo\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones iniciales\n",
    "\n",
    "- Se adaptan los nombres de las columnas para que sean legibles\n",
    "\n",
    "- No se detectan nulos\n",
    "\n",
    "- Se encuentran duplicados, pero como todos tienen su propia identificación se asume que cada diamante es único. Por tanto, se conservan\n",
    "\n",
    "- Se detecta que hay diamantes con 0 milimetros de altura, anchura y/o profundidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambios probados\n",
    "\n",
    "- Se detectan una serie de características:\n",
    "\n",
    "1) Varias columnas tienen unos pocos valores atípicos extremadamente altos [\"depth (percentage)\", \"table (percentage)\", \"width (millimeters)\", \"depth (millimeters)\"]\n",
    "\n",
    "2) Como se ha anticipado, varias filas tienen un cero en todas las variables relacionadas con el tamaño de los diamantes, sin contar el peso [\"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "3) Dos columnas con una alta correlación negativa tienen unos pocos \"outliers\" que destacan moderadamente y que, además, son compartidos [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "4) Si se borrasen las filas con ceros del punto 2, todavía quedaría un cero en una fila para el \"length\". Como los diamantes son prácticamente circulares, \"length\" y \"width\" son casi idénticas [\"length (millimeters)\"].\n",
    "\n",
    "5) Si se borrasen las filas con ceros del punto 2 y se solucionase el cero del punto 4, todavía quedaría un cero en una fila para \"depth (millimeters)\". Se descubre que la variable \"depth (percentage)\" se obtiene de dividir el \"depth (millimeters)\" correspondiente entre la media de \"width\" y \"length\" [\"depth (millimeters)\"].\n",
    "\n",
    "6) Se detecta un \"outlier\" en \"lenght\" [\"length (millimeters)\"].\n",
    "\n",
    "7) Varias columnas son asimétricas por la derecha. Con el logaritmo, se centrarían esas distribuciones y deaparecerían \"outliers\" [\"weight (carat)\", \"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "8) Existe un valor atípico en \"weight\". Sin embargo, imputarlo al valor máximo de su \"boxplot\" haría que resurgiese después de un escalado estándar [\"weight (carat)\"].\n",
    "\n",
    "9) Dos columnas tienen una alta correlación negativa que se quiere conservar, y muchos \"outliers\" no compartidos. Se presume que la posición de esos \"outliers\" es la que determina la correlación [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "10) \"depth (millimeters)\" tiene valores atípicos. Esta variable, junto con las demás relacionadas con el tamaño, están tan correlacionadas que casi podría considerarse que son casi la misma variable escalada [\"depth (millimeters)\"].\n",
    "\n",
    "11) Algunas variables tienen valores mucho más altos que el resto.\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "- Con esa información, se prueban una serie de posibles cambios a llevar a cabo durante la fase de \"feature_engineering\". Si se aplica el conjunto de estos cambios, el \"dataframe\" queda libre de nulos completamente:\n",
    "\n",
    "1) Borrado de \"outliers\" extremadamente altos [\"depth (percentage)\", \"table (percentage)\", \"width (millimeters)\", \"depth (millimeters)\"].\n",
    "\n",
    "2) Borrado de filas que tienen 0 en todas las variables relacionadas con el tamaño excepto el peso [\"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "3) Borrado de los \"outliers\" compartidos moderadamente altos [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "4) Asignación del valor con 0 restante en \"length\" al \"width\" correspondiente [\"length (millimeters)\"].\n",
    "\n",
    "5) Asignación del valor con 0 restante de \"depth (millimeters)\" a partir de una operación con el \"length\", el \"width\" y el \"depth (percentage)\" correspondientes [\"depth (millimeters)\"].\n",
    "\n",
    "6) Asignación del \"outlier\" restante del \"length\" al \"width\" correspondiente [\"length (millimeters)\"].\n",
    "\n",
    "7) Transformación con logaritmo [\"weight (carat)\", \"length (millimeters)\", \"width (millimeters)\" y \"depth (millimeters)\"].\n",
    "\n",
    "8) Imputación al siguiente valor más alto [\"weight (carat)\"].\n",
    "\n",
    "9) Imputación a los valores máximos y mínimos del \"boxplot\" [\"depth (percentage)\" y \"table (percentage)\"].\n",
    "\n",
    "10) Neutralización de \"outliers\" con un modelo \"ridge\" [\"depth (millimeters)\"].\n",
    "\n",
    "11) Escalado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cambios no probados\n",
    "\n",
    "- Se detectan una serie de características:\n",
    "\n",
    "12) La variable \"depth (percentage)\" sale de dividir \"depth (millimeters)\" por la media de \"length (millimeters)\" y \"width (millimeters)\". Por tanto, si se hace la operación, el resultado debería coincidir con la columna de \"depth (percentage)\". Sin embargo, no es así. Habrá que probar durante el \"feature engineering\" si utilizar los valores calculados mejora la predicción.\n",
    "\n",
    "13) Las columnas relacionadas con el tamaño tienen una correlación altísima. Quizá los resultados mejoren con borrarlas. Otras columnas tinen una correlación ínfima, cercana a 0, que podría considerarse irrelevante\n",
    "\n",
    "14) Las filas con el valor máximo de \"clarity quality\" (7, variable discreta) no cambian en cuanto a las variables relacionadas con el tamaño si se comparan con las que tienen una \"clarity quality\" un punto por debajo (6). Quizá cabría imputar esos valores 7 de \"clarity quality\" al 6.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature_engineering\n",
    "\n",
    "- Se aplican los cambios probados y no probados durante el EDA intercalando múltiples modelos para ver 1) qué modelos mejoran con qué cambios, y 2) qué modelo es el adecuado para la optimización. Se trabaja con el \"dataset\" de competición, pero como ya se ha anunciado, los resultados son extrapolables al original\n",
    "\n",
    "- Se lleva a cabo en el \"notebook\" homónimo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de cambios\n",
    "\n",
    "- Se reúnen los cambios apuntados en el EDA en subgrupos:\n",
    "\n",
    "a) Borrado: 1, 2 y 3 (borrado parcial: solo 2)\n",
    "\n",
    "b) Asignación: 4, 5, 6 (asignación parcial: solo 4 y 5)\n",
    "\n",
    "c) Logaritmo: 7\n",
    "\n",
    "d) Valor más alto de \"weight\": 8\n",
    "\n",
    "e) Imputaciones \"boxplot\": 9\n",
    "\n",
    "f) Imputaciones \"ridge\": 10\n",
    "\n",
    "g) Escalado \"MinMax\": 11\n",
    "\n",
    "h) Escalad \"Standard\": 11\n",
    "\n",
    "i) Sustitución: 12\n",
    "\n",
    "j) Descarte correlación altísima: 13 -> ningún modelo mejora\n",
    "\n",
    "j) Descarte correlación ínfima: 13\n",
    "\n",
    "k) Imputaciones \"clarity quality\": 14\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "- Cada uno de los modelos mejora con los siguientes cambios:\n",
    "\n",
    "\n",
    "---------- LinearRegression ----------\n",
    "\n",
    "· Borrado\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Logaritmo\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Imputaciones \"clarity quality\"\n",
    "\n",
    "---------- Ridge ----------\n",
    "\n",
    "· Escalado \"MinMax\"\n",
    "\n",
    "· Borrado\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Logaritmo\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Imputaciones \"clarity quality\"\n",
    "\n",
    "---------- KNeighborsRegressor ----------\n",
    "\n",
    "· Escalado \"Standard\"\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Valor más alto de \"weight\"\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Descarte correlación ínfima\n",
    "\n",
    "· Imputaciones \"clarity quality\"\n",
    "\n",
    "---------- SVR ----------\n",
    "\n",
    "· Escalado \"Standard\"\n",
    "\n",
    "· Borrado\n",
    "\n",
    "· Asignación\n",
    "\n",
    "· Logaritmo\n",
    "\n",
    "· Valor más alto de \"weight\"\n",
    "\n",
    "· Imputaciones \"boxplot\"\n",
    "\n",
    "· Imputaciones \"ridge\"\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "· Descarte correlación ínfima\n",
    "\n",
    "---------- DecisionTree ----------\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación parcial\n",
    "\n",
    "---------- RandomForest ----------\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación parcial\n",
    "\n",
    "· Sustitución\n",
    "\n",
    "---------- XGBRegressor ----------\n",
    "\n",
    "· Borrado parcial\n",
    "\n",
    "· Asignación parcial\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de modelos\n",
    "\n",
    "- Estos son los resultados, una vez aplicados los cambios y tuneados algunos hiperparámetros, si se compara la predicción con el \"y_test\":\n",
    "\n",
    "a) \"LinearRegression\" mejora su \"rmse\" de 0.182664 a 0.143851 (-21.24%)\n",
    "\n",
    "b) \"Ridge\" mejora su \"rmse\" de 0.182826 a 0.144234 (-21.11%)\n",
    "\n",
    "c) k vecinos mejora su \"rmse\" de 0.181632 a 0.121120 (-33.32%)\n",
    "\n",
    "d) \"SVR\" mejora su \"rmse\" de 0.189328 a 0.100319 (-47.01%)\n",
    "\n",
    "e) \"DecisionTree\" mejora su \"rmse\" de 0.130480 a 0.109705 (-15.92%)\n",
    "\n",
    "f) \"RandomForest\" mejora su \"rmse\" de 0.094246 a 0.091972 (-2.41%)\n",
    "\n",
    "g) \"XGBRegressor\" mejora su \"rmse\" de 0.090762 a 0.086893 (-4.26%)\n",
    "\n",
    "- Se elige trabajar en la siguiente fase con \"XGB\". Es el mejor con una \"rmse\" de 0.086893, y, como dispone de muchos más hiperparámetros que los probados hasta ahora, probablemente tenga un margen considerable de optimización\n",
    "\n",
    "- El orden, si \"XGB\" (1) no cumpliese o fuera necesario otro modelo, sería: 2) \"RandomForest\"; 3) \"SVR\"; 4) \"DecisionTree\"; 5) k vecinos; 6) regresión lineal; y 7) \"ridge\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target_transformation\n",
    "\n",
    "- Se confirma que la variable \"target\" del \"dataframe\" de competición se ha escalado, y no representa los precios reales de los diamantes\n",
    "\n",
    "- En el \"notebook\" homónimo se investiga si es cierto que los dos \"datasets\" son la misma cosa y qué tipo de escalado se ha usado\n",
    "\n",
    "- El escalado efectuado es el logaritmo. Sin embargo, se han perdido decimales en el proceso. Por tanto, hay que usar el \"dataframe\" de competición para competir, y el original para predecir precios reales con mayor concreción\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images_data_processing\n",
    "\n",
    "- En el \"notebook\" homónimo, se tratan varios \"datasets\" a parte que representan un conjunto de características de imágenes de diamantes. \n",
    "\n",
    "- El proceso es el siguiente:\n",
    "\n",
    "1) Se unen los \"datasets\" en un solo \"dataframe\". Este es el \"dataframe\" que se utilizará para predecir el precio a partir de las imágenes\n",
    "\n",
    "2) Se copia el \"dataframe\" y, mediante diversas técnicas, se adapta el para que contenga las mismas variables que el ya visto en el EDA. Esto se hace con fines meramente exploratorios, y no se detecta nada que impida la predicción del precio a partir de la imagen\n",
    "\n",
    "- Durante el proceso exploratorio se detecta un mismo diamante con dos precios, así que se elimina. Además, se borran los duplicados\n",
    "\n",
    "- Por otro lado, se elimina una coma y se pasa la variable \"target\", el precio, a numérica"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_optimization_price\n",
    "\n",
    "- Donde se experimenta de forma extensiva con los modelos necesarios con tal de obtener los mejores resultados posibles\n",
    "\n",
    "- El primer modelo entrenado (\"XGB\") que sale es el que se usa para la competición\n",
    "\n",
    "- El segundo (también \"xgb\") y el tercero (k vecinos), entrenados con el \"dataframe\" original de diamantes, no el de competición, se usan mediante un \"stacking\" para predecir los precios reales de los diamantes a partir de sus características. Se usan dos porque \"xgb\" por si solo es incapaz de dar resultados correctos si se le pasan, para la predicción, valores por encima o por debajo de los máximos o mínimos de las variables vistas durante el entrenamiento. Eso es un problema recurrente con los modelos de árboles, y \"SVR\" es muy lento como alternativa, así que se usa k vecinos\n",
    "\n",
    "- Así, se hace un \"stacking\" de \"XGB\" y k vecinos para obtener un cuarto modelo, el bueno, para predecir los precios en la \"app\". Ese modelo combina la solidez de \"XGB\" y la flexibilidad de k vecinos para predecir con valores por encima o por debajo de los máximos y mínimos de las variables vistas durante el entrenamiento\n",
    "\n",
    "- El proceso se lleva a cabo en el \"notebook\" homónimo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras para competición\n",
    "\n",
    "- Estos son los hiperparámetros que se añaden al modelo a cada ronda:\n",
    "\n",
    "1) Se prueba sin modificación alguna para tener una base desde la que hacer comparaciones\n",
    "\n",
    "2) Se prueba con los cambios de \"feature engineering\" (borrado parcial y asignación parcial) por la misma razón\n",
    "\n",
    "3) Se obtiene un número de estimadores óptimo como punto de partida y se tunea el \"learning rate\" (\"eta\"). Una vez tuneado, se vuelve a buscar el número óptimo de estimadores\n",
    "\n",
    "4) Se aplican \"constaints\" a la columna \"weight\", ya que solo crece con el precio\n",
    "\n",
    "5) Se establecen un \"subsample\" y \"colsample_bytree\" de 0.8, que es el estándar recomendable, y se tunean \"max_depth\" y \"min_child_weight\"\n",
    "\n",
    "6) Se tunea \"gamma\", parámetro de semiregularización\n",
    "\n",
    "7) Se prueban dos opciones y se tunean: a) \"sampling\" combinado con \"colsample_bytree\", y b) \"sampling_method='gradient_based\" combinado con \"colsample_bytree\". Gana la opción \"a\"\n",
    "\n",
    "8) Se tunean los hiperparámetros de regularización (\"alpha\", \"lambda\" y \"max_delta_step\"), que son los que ayudan a prevenir el \"overfitting\" y el \"underfitting\"\n",
    "\n",
    "9) Se aumenta el número de árboles en paralelo, convirtiendo, a todos los efectos, el \"xgboost\" en un \"random forest\" con \"booster\"\n",
    "\n",
    "10) Se reajusta el número de estimadores una vez más"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado competición\n",
    "\n",
    "- Se superan los puntos del ganador de la competición, con una \"rmse\" de 0.08506 contra 0.08617\n",
    "\n",
    "- Como el modelo predice bien con datos que no ha visto, hace pensar que no existe \"overfitting\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras para visualización A\n",
    "\n",
    "- Se repite el mismo proceso que en el punto anterior, pero esta vez se usa el original en lugar del de competición\n",
    "\n",
    "- Se usa el \"dataset\" original porque los precios son más precisos. Se le aplica el logaritmo a la \"target\" para facilitar el proceso, que es así como venía el de competición, pero se conservan todos los decimales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado A\n",
    "\n",
    "-  Es mejor que el del anterior modelo (normal, ya que se han usado los precios reales)\n",
    "\n",
    "- Al probar con pasarle diferentes combinaciones de valores, se detecta que este modelo no predice bien cuando recibe cifras que superan o que son inferiores a los máximos y mínimos de las variables vistas durante el entrenamiento. Eso ocurre porque los modelos de árboles no son tan precisos en los extremos, y, como se ha notado en \"feature_enginerring\" y en el EDA, la \"r2\" es alta, y hay varias variables con una correlación altísima. Por eso, se crea un modelo B de predicción de precios\n",
    "\n",
    "- Aunque pudiera parecerlo, eso no ocurre porque haya \"overfitting\", pues el modelo \"xgboost\" usado para la competición ha predicho a partir de datos desconocidos estupendamente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras para visualización B\n",
    "\n",
    "- Con tal de predecir el precio de diamantes cuyas características sean superiores o inferiores a los máximos o mínimos de las variables vistas durante el entrenamiento, se elige el siguiente mejor modelo no \"de árboles\" detectado durante en \"feature_engineering\": \"SVR\"\n",
    "\n",
    "- Sin embargo, como su coste computacional es tan elevado, y se presume que será a aún mayor si se lleva a cabo un \"stacking\", se da preferencia el siguiente de la lista: k vecinos\n",
    "\n",
    "- A pesar de lo visto en el \"feature_engineering\", como se pretende aprovechar el modelo para valores muy altos o bajos a la hora de predecir, interesa entrenarlo con \"outliers\". Por tanto, no se le hace más tratamiento que el que ha recibido \"xgboost\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado B\n",
    "\n",
    "- La predicción con el \"y_test\" es peor, lo que confirma que \"xgboost\" trabaja mejor que k vecinos. Sin embargo, es un buen complemento para predecir si los valores que se le pasan están fuera de los máximos y mínimos de las variables vistas en el entrenamiento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Stacking\"\n",
    "\n",
    "- Se combinan las bondades de ambos modelos con un \"stacking\". Para ello se usa un tercer modelo sencillo de regresión lineal\n",
    "\n",
    "- El modelo resultante predice tan bien como \"xboost\" y, además, es capaz de predecir a partir de valores superiores o inferiores a los máximos y mínimos de las variables vistas durante el entrenamiento\n",
    "\n",
    "- Así, este es el modelo que se usa para la predicción de precios en la \"app\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation_analysis\n",
    "\n",
    "- [Se utiliza \"webscrapping\"](https://www.in2013dollars.com/Jewelry/price-inflation/2017-to-2023?amount=326) para obtener automáticamente la cifra del aumento por inflación respecto a 2017, fecha en que se compuso el \"dataframe\" original de precios\n",
    "\n",
    "- En 2023, respecto a 2017, los precios han subido cerca de un 11%\n",
    "\n",
    "- Se implementa este método en la \"app\", tanto para la predicción a partir de características, cuyo \"dataset\" es de 2017, como la de imágenes, de 2022\n",
    "\n",
    "- Se detecta también una alternativa para aproximar la inflación mediante la media de los últimos años, en caso de que la página dejara de funcionar puntualmente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images_processing\n",
    "\n",
    "- En el \"notebook\" homónimo se iguala el contenido de la carpeta de imágenes procesadas a las \"Id\" de diamantes que contiene el \"dataframe\" correspondiente. Esto se hace porque no todas las imágenes son buenas, las hay que están en blanco. Además, se han borrado duplicados en \"images_data_processing\"\n",
    "\n",
    "- Además, se hace una pequeña exploración de las imágenes con una PCA para ver el mínimo número de componentes y que aspecto tienen\n",
    "\n",
    "- Se determina que, para el modelaje con red neuronal, se necesitará un modelo que acepte un \"input\" RGB con más de 150 componentes\n",
    "\n",
    "- Tas trabajar un poco con el siguiente \"notebook\" (el de \"Model_optimization_price\") se descubre que la predicción es terrible. Por tanto, se regresa a este y se borran los \"outliers\" de la variable \"target\", lo que mejora considerablemente los resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model_optimization_image\n",
    "\n",
    "- En el \"notebook\" homónimo, se procura obtener un modelo que pueda predecir el precio de los diamantes a partir de sus imágenes\n",
    "\n",
    "- Al final, la predicción se acaba haciendo a partir de las imágenes y del peso de los diamantes, ya que tiene una gran influencia y el modelo inicial no parece capaz de dar buenos resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo provisional: MobilenetV3Large\n",
    "\n",
    "- Se prueban muchos modelos (VGG16, Resnet50...) con sus diferentes versiones, pero no se guardan porque el código es largo y complicaría innecesariamente la revisión del \"notebook\". La arquitectura y el funcionamiento es similar, solo cambia el tamaño del \"input\"\n",
    "\n",
    "- Al final, se utiliza un modelo MobileNetV3Large, que es potente, relativamente moderno, de una fuente confiable (Google), y que se ha demostrado eficaz para la detección de imágenes\n",
    "\n",
    "- Como es muy lento, se pone en marcha en Google Collab y se aprovecha la GPU\n",
    "\n",
    "- Como la predicción no es muy buena, se considera provisional. Un segundo modelo usará esa predicción junto con el peso de los diamantes para obtener un precio más ajustado a la realidad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo definitivo: SVR\n",
    "\n",
    "- Se decide crear un modelo de \"machine learning\" que trate de mejorar la predicción del precio de los diamantes a partir del peso y de la predicción provisional hecha anteriormente por la red neuronal. Se hace así por ser más transparentes que las redes neuronales, lo que permite elegir uno u otro en función del comportamiento de los datos más fácilmente\n",
    "\n",
    "- Se hace un \"baseline\" de modelos supervisados, se encuentra que SVR es el más adecuado por tener buenas métricas y poca variación, y se optimiza con el \"kernel\" lineal para aproximar el aumento del precio de los diamantes al subir su peso, así como para tener en cuenta que la predicción final debería crecer a medida que la provisional aumenta\n",
    "\n",
    "- Si bien el precio de los diamantes en relación a su peso es exponencial, eso solo es determinante cuando el peso de los diamantes es elevado. Para diamantes pequeños, puede considerarse que es casi lineal; de ahí que se seleccione SVR con ese \"kernel\" (los diamantes de los que se dispone no son muy grandes en su mayoría)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre la \"app\"\n",
    "\n",
    "- La mayoría de los detalles de su funcionamiento ya se han dado en los puntos anteriores o pueden verse en el desplegable \"a peek into the process\" dentro de la propia \"app\"\n",
    "\n",
    "- La predicción de diamantes de peso mayor 2 quilates se complica (tanto si se hace respecto a las imágenes como con las características). Eso se debe a que los datos de los que se dispone tienen pocos diamantes grandes y, además, no consideran algunas características (como la simetría o la fluorescencia) que, si bien pueden obviarse en los diamantes pequeños, su importancia crece considerablemente con los mayores\n",
    "\n",
    "- Por esa razón, a la hora de hacer la \"app\" se limita la introducción de quilates a 2. Eso impide que las predicciones sean desastrosas y, al mismo tiempo, permite seguir teniendo en cuenta la mayoría de diamantes que hay en el mercado, que son más bien pequeños\n",
    "\n",
    "- Con todo lo hecho, se crea la \"app\" en Streamlit. Puede verse [aquí](https://rogerperello-machine-learning-diamon--streamlitfilesmain-bke5k8.streamlit.app/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consideraciones finales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Que problema hay que resolver?\n",
    "\n",
    "- Evaluar el precio de un diamante no es cosa fácil, y cuensta entre 50 y 150 dólares si se deja en manos de un profesional. Si el diamante es pequeño, eso puede ser un porcentaje importante de su valor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Que solución se aporta?\n",
    "\n",
    "- Se ofrece una aproximación del precio de un diamante a partir de sus características o de su imagen, siempre y cuando este no supere los 2 quilates. Para los que son más grandes es recomendable acudir a un profesional; como el precio del diamante será mayor, saldrá a cuenta el servicio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué modelos se han probado?\n",
    "\n",
    "- Muchísimos. Para predecir el precio a partir de las características:\n",
    "\n",
    "1) Se ha hecho un baseline con \"LinearRegression\", \"Ridge\", k vecinos, \"SVR\", \"DecisionTree\", \"RandomForest\" y \"XGBRegressor\"\n",
    "\n",
    "2) Se ha hecho un \"stacking\" con los modelos tuneados de \"XGBRegressor\" y k vecinos, unidos por una regresión lineal\n",
    "\n",
    "Para predecir el precio a partir de las imágenes:\n",
    "\n",
    "1) Se han probado varios modelos de \"transfer learning\" para obtener un precio provisional (VGG16, Resnet50, mobilenet...), así como varias versiones de los mismos. Se ha elegido, al final, MobilenetV3Large\n",
    "\n",
    "2) Se han probado varios modelos de \"machine learning\" para mejorar esa predicción al juntarla con el peso de los diamantes en quilates. El baseline ha incluido \"LinearRegression\", \"Ridge\", k vecinos, \"SVR\", \"DecisionTree\", \"RandomForest\" y \"XGBRegressor\". Se ha elegido \"SVR\" por su buen rendimiento y se ha tuneado con el \"kernel\" lineal, entre otros hiperparámetros, por razones ya explicadas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué resultados y conclusiones se han obtenido?\n",
    "\n",
    "- Es muy complicado obtener una predicción del precio de un diamante, pues hay muchos factores que no pueden tenerse en cuenta, como la moda actual (por ejemplo, es posible que un determinado color suba puntualmente de precio) o el criterio del vendedor. Además, los datos de los que se dispone son limitados, y no incluyen factores como la fluorescencia o la simetría. Con todo, es factible obtener una predicción que no sea escandalosa para diamantes que no sean exageradamente grandes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cuáles han sido las variables de mayor impacto?\n",
    "\n",
    "- Con diferencia, el principal factor que determina el precio de un diamante es el peso"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué consecuencias tiene en términos de negocio?\n",
    "\n",
    "- La predicción obtenida probablemente no sirva para tasar directamente el diamante, pero sí para determinar, en función del criterio del usuario, si supera un varemo que justifique su venta, o para corroborar una estimación previa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo podría mejorarse?\n",
    "\n",
    "- Sin recurrir a Kaggle, ignorando la competición, haciendo el \"webscrapping\" directamente para obtener fotos y características. Se elegiría una página como [bluenile](https://www.bluenile.com/es/en/diamond-details/LD21034945?track=FCOM&slot=1&type=1), donde los diamantes pueden observarse en diferentes perspectivas, para poder evaluar también la profundidad y detectar más imperfecciones. Además, se extraerían características de las que actualmente no se dispone, como la simetría y el \"culet\"\n",
    "\n",
    "- Obteniendo una muestra mayor, que incluyese más diamantes con precios altos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62be4b4d7aada9f05487a097e316e83dc3ceda15568e9d0ea281b513767b88d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
