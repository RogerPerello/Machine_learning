{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if str(os.getcwdb()[-3:]).split(\"'\")[1] != 'src':\n",
    "    os.chdir(os.path.dirname(os.getcwdb()))\n",
    "\n",
    "from utils.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "\n",
    "    def __init__(self, df, target_name, index=None):\n",
    "        self.target_name = target_name\n",
    "        self.index = index\n",
    "        self.df = df\n",
    "\n",
    "\n",
    "    @property\n",
    "    def dataframe(self):\n",
    "        if self.index:\n",
    "            return self.df.set_index(self.index)\n",
    "        else:\n",
    "            return self.df\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def send_pickle():\n",
    "        pass\n",
    "\n",
    "\n",
    "    def split_dataframe(self, train_num=0.7, random_num=43, scaler=None, return_entire_Xy=False):\n",
    "        self.random_num = random_num\n",
    "        X = self.dataframe.drop(columns=self.target_name)\n",
    "        y = self.dataframe[self.target_name]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, train_size=train_num, random_state=self.random_num)\n",
    "        if scaler:\n",
    "            self.scaler = eval(scaler + '()')\n",
    "            self.scaler_name = ' (' + scaler + ')'\n",
    "            self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "            self.X_test = self.scaler.transform(self.X_test)        \n",
    "            if return_entire_Xy:\n",
    "                self.scaler = eval(scaler + '()')\n",
    "                X = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            self.scaler_name = ''\n",
    "        if return_entire_Xy:\n",
    "            return (X, y)\n",
    "        else:\n",
    "            return (self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "\n",
    "\n",
    "    def prepare_models(self, selected_list=None, excluded_list=None, params_list=None):\n",
    "        self.models = self.chosen_models.copy()\n",
    "        if not excluded_list:\n",
    "            excluded_list = []\n",
    "        if not selected_list:\n",
    "            selected_list = []\n",
    "        self.models_previous = self.models.copy()\n",
    "        for element in self.models_previous.keys():\n",
    "            if (len(selected_list) >= 1 and element not in selected_list) or element in excluded_list:\n",
    "                self.models.pop(element)\n",
    "        for model_name in self.models.keys():\n",
    "            self.models[model_name] = eval(model_name + '()')\n",
    "        if params_list:\n",
    "            for params in params_list:\n",
    "                self.models[params[0] + ': ' + params[1]] = eval(params[0] + '(' + params[1] + ')')\n",
    "            for params in params_list:\n",
    "                    if params[0] in self.models:\n",
    "                        try:\n",
    "                            self.models.pop(params[0])\n",
    "                        except Exception:\n",
    "                            continue\n",
    "        return 'Models prepared. Apply them or use kfold (apply + evaluate)'\n",
    "\n",
    "\n",
    "    def apply_models(self):\n",
    "        print(f'-- {self.type.capitalize()} --')\n",
    "        current_time = time.time()\n",
    "        total_time = time.time() - current_time\n",
    "        for model_name, model in self.models.items():\n",
    "            start_time = time.time()\n",
    "            print(f'Starting {model_name}:')\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            self.y_pred = model.predict(self.X_test)\n",
    "            self.models[model_name] = {'test': np.array(self.y_test), 'prediction': self.y_pred, 'model': model}\n",
    "            execution_time = time.time() - start_time\n",
    "            total_time += execution_time\n",
    "            print(f'- {model_name} done in {round(execution_time, 2)} sec(s). Total time: {round(total_time, 2)}')\n",
    "        return self.models\n",
    "\n",
    "\n",
    "    def create_dataframe(self, best_values_list, worst_values_list):\n",
    "        self.df = pd.DataFrame(data=self.models_metrics)\n",
    "        if best_values_list:\n",
    "            best_values_list = [element[0] for element in best_values_list]\n",
    "            worst_values_list = [element[0] for element in worst_values_list]\n",
    "            self.df['BEST'] = best_values_list\n",
    "            self.df['WORST'] = worst_values_list\n",
    "\n",
    "\n",
    "    def visualize(self, metrics_selection=None):\n",
    "        visualization_dict = {'models': [model_name for model_name in self.models_metrics.keys() for metric in self.models_metrics[model_name] if (not metrics_selection or metric in metrics_selection)],\n",
    "                              'metrics': [metric for model_name in self.models_metrics.keys() for metric in self.models_metrics[model_name] if (not metrics_selection or metric in metrics_selection)],\n",
    "                              'values': [self.models_metrics[model_name][metric] for model_name in self.models_metrics.keys() for metric in self.models_metrics[model_name] if (not metrics_selection or metric in metrics_selection)]\n",
    "                              }\n",
    "        sns.lineplot(data=visualization_dict, x='models', y='values', hue='metrics')\n",
    "        plt.tick_params(axis='x', labelrotation = 30)\n",
    "        plt.title('Medicine price by date')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class Regression(Model):\n",
    "    chosen_models = dict()\n",
    "\n",
    "\n",
    "    def __init__(self, dataframe, target_name, index=None):\n",
    "        super().__init__(dataframe, target_name, index)\n",
    "        self.type = 'regression'\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def add_models(cls, regression_list):\n",
    "        if regression_list:\n",
    "            for element in regression_list:\n",
    "                cls.chosen_models[element] = ''\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def remove_models(cls, regression_list):\n",
    "        if regression_list:\n",
    "            for element in regression_list:\n",
    "                cls.chosen_models.pop(element)\n",
    "\n",
    "  \n",
    "    def apply_and_evaluate_kfolds(self, kfolds_num=5):\n",
    "        self.kfolds_num = kfolds_num\n",
    "        self.kfolds = KFold(n_splits=kfolds_num, shuffle=True, random_state=self.random_num)\n",
    "        self.kfold = 'fold'\n",
    "        metrics = ['neg_root_mean_squared_error', 'neg_mean_squared_error', 'neg_mean_absolute_error', 'r2', 'neg_mean_absolute_percentage_error']\n",
    "        self.models_evaluated = dict()\n",
    "        print(f'-- {self.type.capitalize()}{self.scaler_name}: using mean of {self.kfolds_num} {self.kfold}s --')\n",
    "        current_time = time.time()\n",
    "        total_time = time.time() - current_time\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f'Starting {model_name}:')\n",
    "            start_time = time.time()\n",
    "            cross_val = cross_validate(model, self.X_train, self.y_train, cv=self.kfolds, return_estimator=True, scoring=metrics)\n",
    "            list_of_metrics = list(cross_val.keys())[3:]\n",
    "            self.models_evaluated[model_name] = dict()\n",
    "            self.models_evaluated[model_name]['models'] = cross_val['estimator']\n",
    "            self.models_evaluated[model_name]['metrics'] = {'rmse': abs(np.mean(list(cross_val.values())[3:][0])), \n",
    "                                                            'mse': abs(np.mean(list(cross_val.values())[3:][1])), \n",
    "                                                            'mae': abs(np.mean(list(cross_val.values())[3:][2])), \n",
    "                                                            'r2_score': np.mean(list(cross_val.values())[3:][3]), \n",
    "                                                            'mape': abs(np.mean(list(cross_val.values())[3:][4]))}\n",
    "            self.models_evaluated[model_name]['all_metrics'] = {'rmse': list(map(abs, list(cross_val.values())[3:][0])), \n",
    "                                                            'mse': list(map(abs, list(cross_val.values())[3:][1])), \n",
    "                                                            'mae': list(map(abs, list(cross_val.values())[3:][2])), \n",
    "                                                            'r2_score': list(map(abs, list(cross_val.values())[3:][3])), \n",
    "                                                            'mape': list(map(abs, list(cross_val.values())[3:][4]))}\n",
    "            self.models_evaluated[model_name]['variances'] = {'rmse': np.var(list(cross_val.values())[3:][0]), \n",
    "                                                            'mse': np.var(list(cross_val.values())[3:][1]), \n",
    "                                                            'mae': np.var(list(cross_val.values())[3:][2]), \n",
    "                                                            'r2_score': np.var(list(cross_val.values())[3:][3]), \n",
    "                                                            'mape': np.var(list(cross_val.values())[3:][4])}\n",
    "            execution_time = time.time() - start_time\n",
    "            total_time += execution_time\n",
    "            print(f'- {model_name} done in {round(execution_time, 2)} sec(s). Total time: {round(total_time, 2)}')\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def evaluate_metrics(self):\n",
    "        self.models_evaluated = self.models.copy()\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            rmse = mean_squared_error(model_results['test'], model_results['prediction'], squared=False)\n",
    "            mse = mean_squared_error(model_results['test'], model_results['prediction'])\n",
    "            mae = mean_absolute_error(model_results['test'], model_results['prediction'])\n",
    "            r2 = r2_score(model_results['test'], model_results['prediction'])\n",
    "            mape = mean_absolute_percentage_error(model_results['test'], model_results['prediction'])\n",
    "            self.models_evaluated[model_name]['metrics'] = {'rmse': rmse, 'mse': mse, 'mae': mae, 'r2_score': r2, 'mape': mape}\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def create_dataframe(self, chosen_metric='mean'):\n",
    "        self.models_metrics = self.models_evaluated.copy()\n",
    "        best_values_list = []\n",
    "        worst_values_list = []\n",
    "        if chosen_metric == 'mean':\n",
    "            chosen_metric = 'metrics'\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            self.models_metrics[model_name] = self.models_metrics[model_name][chosen_metric]\n",
    "            if len(self.models_metrics) > 1:\n",
    "                model_values = [value if type(value) is not list else sum([row[index] for index, row in enumerate(value)]) for value in self.models_evaluated[model_name][chosen_metric].values()]\n",
    "                if not best_values_list:\n",
    "                    best_values_list = [[model_name, value] for value in model_values]\n",
    "                    worst_values_list = [[model_name, value] for value in model_values]\n",
    "                else:\n",
    "                    for index, value in enumerate(model_values):\n",
    "                        if value < best_values_list[index][1]:\n",
    "                            if index != 3:\n",
    "                                best_values_list[index][1] = value\n",
    "                                best_values_list[index][0] = model_name\n",
    "                            else:\n",
    "                                worst_values_list[index][1] = value\n",
    "                                worst_values_list[index][0] = model_name\n",
    "                        if value > worst_values_list[index][1]:\n",
    "                            if index != 3:\n",
    "                                worst_values_list[index][1] = value\n",
    "                                worst_values_list[index][0] = model_name\n",
    "                            else:\n",
    "                                best_values_list[index][1] = value\n",
    "                                best_values_list[index][0] = model_name\n",
    "        super().create_dataframe(best_values_list, worst_values_list)\n",
    "        return self.df\n",
    "\n",
    "\n",
    "\n",
    "class Classification(Model):\n",
    "    chosen_models = dict()\n",
    "\n",
    "\n",
    "    def __init__(self, dataframe, target_name, index=None):\n",
    "        super().__init__(dataframe, target_name, index)\n",
    "        self.type = 'classification'\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def add_models(cls, classification_list):\n",
    "        if classification_list:\n",
    "            for element in classification_list:\n",
    "                cls.chosen_models[element] = ''\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def remove_models(cls, classification_list):\n",
    "        if classification_list:\n",
    "            for element in classification_list:\n",
    "                cls.chosen_models.pop(element)\n",
    "\n",
    "\n",
    "    def apply_and_evaluate_kfolds(self, kfolds_num=5, multiclass_average=None):\n",
    "        self.kfolds = StratifiedKFold(n_splits=kfolds_num, shuffle=True, random_state=self.random_num)\n",
    "        self.kfolds_num = kfolds_num\n",
    "        self.kfold = 'stratified fold'\n",
    "        metrics = ['accuracy', 'recall', 'precision', 'f1']\n",
    "        if multiclass_average == 'micro':\n",
    "            metrics = ['accuracy', 'precision_micro', 'recall_micro', 'f1_micro'] \n",
    "        elif multiclass_average == 'macro':\n",
    "            metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'] \n",
    "        elif multiclass_average == 'samples':\n",
    "            metrics = ['accuracy', 'precision_samples', 'recall_samples', 'f1_samples'] \n",
    "        elif multiclass_average == 'weighted':\n",
    "            metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'] \n",
    "        self.models_evaluated = dict()\n",
    "        print(f'-- {self.type.capitalize()}{self.scaler_name}: using mean of {self.kfolds_num} {self.kfold}s --')\n",
    "        current_time = time.time()\n",
    "        total_time = time.time() - current_time\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f'Starting {model_name}:')\n",
    "            start_time = time.time()\n",
    "            cross_val = cross_validate(model, self.X_train, self.y_train, cv=self.kfolds, return_estimator=True, scoring=metrics)\n",
    "            self.models_evaluated[model_name] = dict()\n",
    "            self.models_evaluated[model_name]['models'] = cross_val['estimator']\n",
    "            self.models_evaluated[model_name]['metrics'] = {'accuracy': abs(np.mean(list(cross_val.values())[3:][0])), \n",
    "                                                            'recall': abs(np.mean(list(cross_val.values())[3:][1])), \n",
    "                                                            'precision': abs(np.mean(list(cross_val.values())[3:][2])), \n",
    "                                                            'f1_score': np.mean(list(cross_val.values())[3:][3])}\n",
    "            self.models_evaluated[model_name]['all_metrics'] = {'accuracy': list(map(abs, list(cross_val.values())[3:][0])), \n",
    "                                                            'recall': list(map(abs, list(cross_val.values())[3:][1])), \n",
    "                                                            'precision': list(map(abs, list(cross_val.values())[3:][2])), \n",
    "                                                            'f1_score': list(map(abs, list(cross_val.values())[3:][3]))}\n",
    "            self.models_evaluated[model_name]['variances'] = {'accuracy': np.var(list(cross_val.values())[3:][0]), \n",
    "                                                            'recall': np.var(list(cross_val.values())[3:][1]), \n",
    "                                                            'precision': np.var(list(cross_val.values())[3:][2]), \n",
    "                                                            'f1_score': np.var(list(cross_val.values())[3:][3])}\n",
    "            execution_time = time.time() - start_time\n",
    "            total_time += execution_time\n",
    "            print(f'- {model_name} done in {round(execution_time, 2)} sec(s). Total time: {round(total_time, 2)}')\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def evaluate_metrics(self, params_list=None):\n",
    "        self.models_evaluated = self.models.copy()\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            accuracy = \"accuracy_score (model_results['test'], model_results['prediction']\"\n",
    "            recall = \"recall_score (model_results['test'], model_results['prediction']\"\n",
    "            precision = \"precision_score (model_results['test'], model_results['prediction']\"\n",
    "            f1 = \"f1_score (model_results['test'], model_results['prediction']\"\n",
    "            matrix = \"confusion_matrix (model_results['test'], model_results['prediction']\"\n",
    "            list_of_metrics = []\n",
    "            for index, element in enumerate([accuracy, recall, precision, f1, matrix], 1):\n",
    "                if params_list:\n",
    "                    for params in params_list:\n",
    "                        if params[0] == element.split()[0]:\n",
    "                            element += ', ' + params[1] + ')'\n",
    "                if element[-1] == ']':\n",
    "                    element += ')'\n",
    "                list_of_metrics.append(eval(element))\n",
    "            print(list_of_metrics)\n",
    "            confusion = [element for element in list_of_metrics[-1]]\n",
    "            self.models_evaluated[model_name]['metrics'] = {'accuracy': list_of_metrics[0], 'recall': list_of_metrics[1], 'precision': list_of_metrics[2], 'f1_score': list_of_metrics[3], 'confusion_matrix': confusion}\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def create_dataframe(self, chosen_metric='mean'):\n",
    "        self.models_metrics = self.models_evaluated.copy()\n",
    "        best_values_list = []\n",
    "        worst_values_list = []\n",
    "        if chosen_metric == 'mean':\n",
    "            chosen_metric = 'metrics'\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            self.models_metrics[model_name] = self.models_metrics[model_name][chosen_metric]\n",
    "            if len(self.models_metrics) > 1:\n",
    "                model_values = [value if type(value) is not list else sum([row[index] for index, row in enumerate(value)]) for value in self.models_evaluated[model_name][chosen_metric].values()]\n",
    "                if not best_values_list:\n",
    "                    best_values_list = [[model_name, value] for value in model_values]\n",
    "                    worst_values_list = [[model_name, value] for value in model_values]\n",
    "                else:\n",
    "                    for index, value in enumerate(model_values):\n",
    "                        if value > best_values_list[index][1]:\n",
    "                            best_values_list[index][1] = value\n",
    "                            best_values_list[index][0] = model_name\n",
    "                        if value < worst_values_list[index][1]:\n",
    "                            worst_values_list[index][1] = value\n",
    "                            worst_values_list[index][0] = model_name\n",
    "        super().create_dataframe(best_values_list, worst_values_list)\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diamonds = pd.read_csv(r'data\\processed\\diamonds_training.csv', index_col='id')\n",
    "df_predict = pd.read_csv(r'data\\processed\\diamonds_testing.csv', index_col='id')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62be4b4d7aada9f05487a097e316e83dc3ceda15568e9d0ea281b513767b88d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
