{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if str(os.getcwdb()[-3:]).split(\"'\")[1] != 'src':\n",
    "    os.chdir(os.path.dirname(os.getcwdb()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import copy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix, f1_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "\n",
    "    def __init__(self, df, target_name, index=None):\n",
    "        self.target_name = target_name\n",
    "        self.index = index\n",
    "        self.df = df\n",
    "\n",
    "\n",
    "    @property\n",
    "    def dataframe(self):\n",
    "        if self.index:\n",
    "            return self.df.set_index(self.index)\n",
    "        else:\n",
    "            return self.df\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def send_pickle():\n",
    "        pass\n",
    "\n",
    "\n",
    "    def split_dataframe(self, train_num=0.7, random_num=43, scaler=None):\n",
    "        self.random_num = random_num\n",
    "        X = self.dataframe.drop(columns=self.target_name)\n",
    "        y = self.dataframe[self.target_name]\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, train_size=train_num, random_state=self.random_num)\n",
    "        if scaler:\n",
    "            self.scaler = eval(scaler + '()')\n",
    "            self.scaler_name = ' (' + scaler + ')'\n",
    "            self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "            self.X_test = self.scaler.transform(self.X_test)        \n",
    "        else:\n",
    "            self.scaler_name = ''\n",
    "        return (self.X_train, self.X_test, self.y_train, self.y_test)\n",
    "\n",
    "\n",
    "    def prepare_models(self, selected_list=None, excluded_list=None, params_list=None):\n",
    "        self.models = self.chosen_models.copy()\n",
    "        if not excluded_list:\n",
    "            excluded_list = []\n",
    "        if not selected_list:\n",
    "            selected_list = []\n",
    "        self.models_previous = self.models.copy()\n",
    "        for element in self.models_previous.keys():\n",
    "            if (len(selected_list) >= 1 and element not in selected_list) or element in excluded_list:\n",
    "                self.models.pop(element)\n",
    "        for model_name in self.models.keys():\n",
    "            self.models[model_name] = eval(model_name + '()')\n",
    "        if params_list:\n",
    "            for params in params_list:\n",
    "                self.models[params[0] + ': ' + params[1]] = eval(params[0] + '(' + params[1] + ')')\n",
    "            for params in params_list:\n",
    "                    if params[0] in self.models:\n",
    "                        try:\n",
    "                            self.models.pop(params[0])\n",
    "                        except Exception:\n",
    "                            continue\n",
    "        return 'Models prepared. Apply them or use kfold (apply + evaluate)'\n",
    "\n",
    "\n",
    "    def apply_models(self):\n",
    "        print(f'-- {self.type.capitalize()} --')\n",
    "        current_time = time.time()\n",
    "        total_time = time.time() - current_time\n",
    "        for model_name, model in self.models.items():\n",
    "            start_time = time.time()\n",
    "            print(f'Starting {model_name}:')\n",
    "            model.fit(self.X_train, self.y_train)\n",
    "            self.y_pred = model.predict(self.X_test)\n",
    "            self.models[model_name] = {'test': np.array(self.y_test), 'prediction': self.y_pred, 'model': model}\n",
    "            execution_time = time.time() - start_time\n",
    "            total_time += execution_time\n",
    "            print(f'- {model_name} done in {round(execution_time, 2)} sec(s). Total time: {round(total_time, 2)}')\n",
    "        return self.models\n",
    "\n",
    "\n",
    "    def create_dataframe(self, best_values_list, worst_values_list, models_metrics):\n",
    "        self.df = pd.DataFrame(data=models_metrics)\n",
    "        if best_values_list:\n",
    "            best_values_list = [element[0] for element in best_values_list]\n",
    "            worst_values_list = [element[0] for element in worst_values_list]\n",
    "            self.df['BEST'] = best_values_list\n",
    "            self.df['WORST'] = worst_values_list\n",
    "\n",
    "\n",
    "\n",
    "class Regression(Model):\n",
    "    chosen_models = dict()\n",
    "\n",
    "\n",
    "    def __init__(self, dataframe, target_name, index=None):\n",
    "        super().__init__(dataframe, target_name, index)\n",
    "        self.type = 'regression'\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def add_models(cls, regression_list):\n",
    "        if regression_list:\n",
    "            for element in regression_list:\n",
    "                cls.chosen_models[element] = ''\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def remove_models(cls, regression_list):\n",
    "        if regression_list:\n",
    "            for element in regression_list:\n",
    "                cls.chosen_models.pop(element)\n",
    "\n",
    "  \n",
    "    def apply_and_evaluate_kfolds(self, kfolds_num=5):\n",
    "        self.kfolds_num = kfolds_num\n",
    "        self.kfolds = KFold(n_splits=kfolds_num, shuffle=True, random_state=self.random_num)\n",
    "        self.kfold = 'fold'\n",
    "        metrics = ['neg_root_mean_squared_error', 'neg_mean_squared_error', 'neg_mean_absolute_error', 'r2', 'neg_mean_absolute_percentage_error']\n",
    "        self.models_evaluated = dict()\n",
    "        print(f'-- {self.type.capitalize()}{self.scaler_name}: using mean of {self.kfolds_num} {self.kfold}s --')\n",
    "        current_time = time.time()\n",
    "        total_time = time.time() - current_time\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f'Starting {model_name}:')\n",
    "            start_time = time.time()\n",
    "            cross_val = cross_validate(model, self.X_train, self.y_train, cv=self.kfolds, return_estimator=True, scoring=metrics)\n",
    "            list_of_metrics = list(cross_val.keys())[3:]\n",
    "            self.models_evaluated[model_name] = dict()\n",
    "            self.models_evaluated[model_name]['models'] = cross_val['estimator']\n",
    "            self.models_evaluated[model_name]['metrics'] = {'rmse': abs(np.mean(list(cross_val.values())[3:][0])), \n",
    "                                                            'mse': abs(np.mean(list(cross_val.values())[3:][1])), \n",
    "                                                            'mae': abs(np.mean(list(cross_val.values())[3:][2])), \n",
    "                                                            'r2_score': np.mean(list(cross_val.values())[3:][3]), \n",
    "                                                            'mape': abs(np.mean(list(cross_val.values())[3:][4]))}\n",
    "            self.models_evaluated[model_name]['all_metrics'] = {'rmse': list(map(abs, list(cross_val.values())[3:][0])), \n",
    "                                                            'mse': list(map(abs, list(cross_val.values())[3:][1])), \n",
    "                                                            'mae': list(map(abs, list(cross_val.values())[3:][2])), \n",
    "                                                            'r2_score': list(map(abs, list(cross_val.values())[3:][3])), \n",
    "                                                            'mape': list(map(abs, list(cross_val.values())[3:][4]))}\n",
    "            self.models_evaluated[model_name]['variances'] = {'rmse': np.var(list(cross_val.values())[3:][0]), \n",
    "                                                            'mse': np.var(list(cross_val.values())[3:][1]), \n",
    "                                                            'mae': np.var(list(cross_val.values())[3:][2]), \n",
    "                                                            'r2_score': np.var(list(cross_val.values())[3:][3]), \n",
    "                                                            'mape': np.var(list(cross_val.values())[3:][4])}\n",
    "            execution_time = time.time() - start_time\n",
    "            total_time += execution_time\n",
    "            print(f'- {model_name} done in {round(execution_time, 2)} sec(s). Total time: {round(total_time, 2)}')\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def evaluate_metrics(self):\n",
    "        self.models_evaluated = self.models.copy()\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            rmse = mean_squared_error(model_results['test'], model_results['prediction'], squared=False)\n",
    "            mse = mean_squared_error(model_results['test'], model_results['prediction'])\n",
    "            mae = mean_absolute_error(model_results['test'], model_results['prediction'])\n",
    "            r2 = r2_score(model_results['test'], model_results['prediction'])\n",
    "            mape = mean_absolute_percentage_error(model_results['test'], model_results['prediction'])\n",
    "            self.models_evaluated[model_name]['metrics'] = {'rmse': rmse, 'mse': mse, 'mae': mae, 'r2_score': r2, 'mape': mape}\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        models_metrics = self.models_evaluated.copy()\n",
    "        best_values_list = []\n",
    "        worst_values_list = []\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            models_metrics[model_name] = models_metrics[model_name]['metrics']\n",
    "            if len(models_metrics) > 1:\n",
    "                model_values = [value if type(value) is not list else sum([row[index] for index, row in enumerate(value)]) for value in self.models_evaluated[model_name]['metrics'].values()]\n",
    "                if not best_values_list:\n",
    "                    best_values_list = [[model_name, value] for value in model_values]\n",
    "                    worst_values_list = [[model_name, value] for value in model_values]\n",
    "                else:\n",
    "                    for index, value in enumerate(model_values):\n",
    "                        if value < best_values_list[index][1]:\n",
    "                            if index != 3:\n",
    "                                best_values_list[index][1] = value\n",
    "                                best_values_list[index][0] = model_name\n",
    "                            else:\n",
    "                                worst_values_list[index][1] = value\n",
    "                                worst_values_list[index][0] = model_name\n",
    "                        if value > worst_values_list[index][1]:\n",
    "                            if index != 3:\n",
    "                                worst_values_list[index][1] = value\n",
    "                                worst_values_list[index][0] = model_name\n",
    "                            else:\n",
    "                                best_values_list[index][1] = value\n",
    "                                best_values_list[index][0] = model_name\n",
    "        super().create_dataframe(best_values_list, worst_values_list, models_metrics)\n",
    "        return self.df\n",
    "\n",
    "\n",
    "class Classification(Model):\n",
    "    chosen_models = dict()\n",
    "\n",
    "\n",
    "    def __init__(self, dataframe, target_name, index=None):\n",
    "        super().__init__(dataframe, target_name, index)\n",
    "        self.type = 'classification'\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def add_models(cls, classification_list):\n",
    "        if classification_list:\n",
    "            for element in classification_list:\n",
    "                cls.chosen_models[element] = ''\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def remove_models(cls, classification_list):\n",
    "        if classification_list:\n",
    "            for element in classification_list:\n",
    "                cls.chosen_models.pop(element)\n",
    "\n",
    "\n",
    "    def apply_and_evaluate_kfolds(self, kfolds_num=5, multiclass_average=None):\n",
    "        self.kfolds = StratifiedKFold(n_splits=kfolds_num, shuffle=True, random_state=self.random_num)\n",
    "        self.kfolds_num = kfolds_num\n",
    "        self.kfold = 'stratified fold'\n",
    "        metrics = ['accuracy', 'recall', 'precision', 'f1']\n",
    "        if multiclass_average == 'micro':\n",
    "            metrics = ['accuracy', 'precision_micro', 'recall_micro', 'f1_micro'] \n",
    "        elif multiclass_average == 'macro':\n",
    "            metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'] \n",
    "        elif multiclass_average == 'samples':\n",
    "            metrics = ['accuracy', 'precision_samples', 'recall_samples', 'f1_samples'] \n",
    "        elif multiclass_average == 'weighted':\n",
    "            metrics = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted'] \n",
    "        self.models_evaluated = dict()\n",
    "        print(f'-- {self.type.capitalize()}{self.scaler_name}: using mean of {self.kfolds_num} {self.kfold}s --')\n",
    "        current_time = time.time()\n",
    "        total_time = time.time() - current_time\n",
    "        for model_name, model in self.models.items():\n",
    "            print(f'Starting {model_name}:')\n",
    "            start_time = time.time()\n",
    "            cross_val = cross_validate(model, self.X_train, self.y_train, cv=self.kfolds, return_estimator=True, scoring=metrics)\n",
    "            self.models_evaluated[model_name] = dict()\n",
    "            self.models_evaluated[model_name]['models'] = cross_val['estimator']\n",
    "            self.models_evaluated[model_name]['metrics'] = {'accuracy': abs(np.mean(list(cross_val.values())[3:][0])), \n",
    "                                                            'recall': abs(np.mean(list(cross_val.values())[3:][1])), \n",
    "                                                            'precision': abs(np.mean(list(cross_val.values())[3:][2])), \n",
    "                                                            'f1_score': np.mean(list(cross_val.values())[3:][3])}\n",
    "            self.models_evaluated[model_name]['all_metrics'] = {'accuracy': list(map(abs, list(cross_val.values())[3:][0])), \n",
    "                                                            'recall': list(map(abs, list(cross_val.values())[3:][1])), \n",
    "                                                            'precision': list(map(abs, list(cross_val.values())[3:][2])), \n",
    "                                                            'f1_score': list(map(abs, list(cross_val.values())[3:][3]))}\n",
    "            self.models_evaluated[model_name]['variances'] = {'accuracy': np.var(list(cross_val.values())[3:][0]), \n",
    "                                                            'recall': np.var(list(cross_val.values())[3:][1]), \n",
    "                                                            'precision': np.var(list(cross_val.values())[3:][2]), \n",
    "                                                            'f1_score': np.var(list(cross_val.values())[3:][3])}\n",
    "            execution_time = time.time() - start_time\n",
    "            total_time += execution_time\n",
    "            print(f'- {model_name} done in {round(execution_time, 2)} sec(s). Total time: {round(total_time, 2)}')\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def evaluate_metrics(self, params_list=None):\n",
    "        self.models_evaluated = self.models.copy()\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            accuracy = \"accuracy_score (model_results['test'], model_results['prediction']\"\n",
    "            recall = \"recall_score (model_results['test'], model_results['prediction']\"\n",
    "            precision = \"precision_score (model_results['test'], model_results['prediction']\"\n",
    "            f1 = \"f1_score (model_results['test'], model_results['prediction']\"\n",
    "            matrix = \"confusion_matrix (model_results['test'], model_results['prediction']\"\n",
    "            list_of_metrics = []\n",
    "            for index, element in enumerate([accuracy, recall, precision, f1, matrix], 1):\n",
    "                if params_list:\n",
    "                    for params in params_list:\n",
    "                        if params[0] == element.split()[0]:\n",
    "                            element += ', ' + params[1] + ')'\n",
    "                if element[-1] == ']':\n",
    "                    element += ')'\n",
    "                list_of_metrics.append(eval(element))\n",
    "            print(list_of_metrics)\n",
    "            confusion = [element for element in list_of_metrics[-1]]\n",
    "            self.models_evaluated[model_name]['metrics'] = {'accuracy': list_of_metrics[0], 'recall': list_of_metrics[1], 'precision': list_of_metrics[2], 'f1_score': list_of_metrics[3], 'confusion_matrix': confusion}\n",
    "        return self.models_evaluated\n",
    "\n",
    "\n",
    "    def create_dataframe(self):\n",
    "        models_metrics = self.models_evaluated.copy()\n",
    "        best_values_list = []\n",
    "        worst_values_list = []\n",
    "        for model_name, model_results in self.models_evaluated.items():\n",
    "            models_metrics[model_name] = models_metrics[model_name]['metrics']\n",
    "            if len(models_metrics) > 1:\n",
    "                model_values = [value if type(value) is not list else sum([row[index] for index, row in enumerate(value)]) for value in self.models_evaluated[model_name]['metrics'].values()]\n",
    "                if not best_values_list:\n",
    "                    best_values_list = [[model_name, value] for value in model_values]\n",
    "                    worst_values_list = [[model_name, value] for value in model_values]\n",
    "                else:\n",
    "                    for index, value in enumerate(model_values):\n",
    "                        if value > best_values_list[index][1]:\n",
    "                            best_values_list[index][1] = value\n",
    "                            best_values_list[index][0] = model_name\n",
    "                        if value < worst_values_list[index][1]:\n",
    "                            worst_values_list[index][1] = value\n",
    "                            worst_values_list[index][0] = model_name\n",
    "        super().create_dataframe(best_values_list, worst_values_list, models_metrics)\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diamonds = pd.read_csv(r'data\\processed\\diamonds_training.csv', index_col='id')\n",
    "df_predict = pd.read_csv(r'data\\processed\\diamonds_testing.csv', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression.add_models(['LinearRegression',\n",
    "                        'XGBRegressor'\n",
    "                        ]\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Regression: using mean of 5 folds --\n",
      "Starting LinearRegression:\n",
      "- LinearRegression done in 0.15 sec(s). Total time: 0.15\n",
      "Starting XGBRegressor:\n",
      "- XGBRegressor done in 5.62 sec(s). Total time: 5.76\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>BEST</th>\n",
       "      <th>WORST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.182664</td>\n",
       "      <td>0.090762</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mse</th>\n",
       "      <td>0.033669</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mae</th>\n",
       "      <td>0.118476</td>\n",
       "      <td>0.065596</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.967188</td>\n",
       "      <td>0.991976</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mape</th>\n",
       "      <td>0.015317</td>\n",
       "      <td>0.008468</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>LinearRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LinearRegression  XGBRegressor          BEST             WORST\n",
       "rmse              0.182664      0.090762  XGBRegressor  LinearRegression\n",
       "mse               0.033669      0.008247  XGBRegressor  LinearRegression\n",
       "mae               0.118476      0.065596  XGBRegressor  LinearRegression\n",
       "r2_score          0.967188      0.991976  XGBRegressor  LinearRegression\n",
       "mape              0.015317      0.008468  XGBRegressor  LinearRegression"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_1 = Regression(df_diamonds, 'price')\n",
    "round_1.split_dataframe()\n",
    "round_1.prepare_models()\n",
    "dict_round_1 = round_1.apply_and_evaluate_kfolds()\n",
    "round_1.create_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LinearRegression': {'models': [LinearRegression(),\n",
       "   LinearRegression(),\n",
       "   LinearRegression(),\n",
       "   LinearRegression(),\n",
       "   LinearRegression()],\n",
       "  'metrics': {'rmse': 0.18266434126360717,\n",
       "   'mse': 0.033668866217605245,\n",
       "   'mae': 0.11847602510487627,\n",
       "   'r2_score': 0.9671883402028569,\n",
       "   'mape': 0.015316754357796062},\n",
       "  'all_metrics': {'rmse': [0.18278436404836218,\n",
       "    0.16134131610876856,\n",
       "    0.21213340614001955,\n",
       "    0.1873559277102721,\n",
       "    0.16970669231061353],\n",
       "   'mse': [0.0334101237405642,\n",
       "    0.026031020283709582,\n",
       "    0.04500058200056649,\n",
       "    0.035102243648176705,\n",
       "    0.02880036141500925],\n",
       "   'mae': [0.1198527764880509,\n",
       "    0.12009375582158771,\n",
       "    0.11814603052977636,\n",
       "    0.1176840754805362,\n",
       "    0.1166034872044301],\n",
       "   'r2_score': [0.9675277226414865,\n",
       "    0.9749723070956142,\n",
       "    0.9554574304863221,\n",
       "    0.9657327540523,\n",
       "    0.9722514867385628],\n",
       "   'mape': [0.015509386119075434,\n",
       "    0.015492198456540348,\n",
       "    0.015267110508885734,\n",
       "    0.015208411444627408,\n",
       "    0.015106665259851387]},\n",
       "  'variances': {'rmse': 0.00030260464833769786,\n",
       "   'mse': 4.2513666127307344e-05,\n",
       "   'mae': 1.750995171712013e-06,\n",
       "   'r2_score': 4.521474953768103e-05,\n",
       "   'mape': 2.5245550976299345e-08}},\n",
       " 'XGBRegressor': {'models': [XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "   XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "   XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "   XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...),\n",
       "   XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                colsample_bylevel=None, colsample_bynode=None,\n",
       "                colsample_bytree=None, early_stopping_rounds=None,\n",
       "                enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "                gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "                interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "                min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "                n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "                predictor=None, random_state=None, ...)],\n",
       "  'metrics': {'rmse': 0.09076170575844941,\n",
       "   'mse': 0.008246528593062819,\n",
       "   'mae': 0.0655959786017388,\n",
       "   'r2_score': 0.9919759161100286,\n",
       "   'mape': 0.008468233206371287},\n",
       "  'all_metrics': {'rmse': [0.08996558722752356,\n",
       "    0.09144750589231505,\n",
       "    0.09615672402069722,\n",
       "    0.08799201872225776,\n",
       "    0.08824669292945346],\n",
       "   'mse': [0.00809380688519315,\n",
       "    0.008362646333924997,\n",
       "    0.00924611557439253,\n",
       "    0.007742595358818161,\n",
       "    0.0077874788129852525],\n",
       "   'mae': [0.06605196058952202,\n",
       "    0.06624132949618969,\n",
       "    0.0663223270218251,\n",
       "    0.0647224877728141,\n",
       "    0.06464178812834308],\n",
       "   'r2_score': [0.9921333921387685,\n",
       "    0.9919596795656741,\n",
       "    0.9908479906837939,\n",
       "    0.9924415823075764,\n",
       "    0.9924969358543304],\n",
       "   'mape': [0.008531660689332372,\n",
       "    0.008543769750799632,\n",
       "    0.008567596644067978,\n",
       "    0.008371561699980611,\n",
       "    0.008326577247675836]},\n",
       "  'variances': {'rmse': 8.841360879468672e-06,\n",
       "   'mse': 3.001313576413901e-07,\n",
       "   'mae': 5.650890130495746e-07,\n",
       "   'r2_score': 3.5711693472757833e-07,\n",
       "   'mape': 9.802739734325601e-09}}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_round_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "\n",
    "X = load_wine(as_frame=True).data\n",
    "y = load_wine(as_frame=True).target\n",
    "\n",
    "df = pd.concat([X, y], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification.add_models(['LogisticRegression',\n",
    "                            ]\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_2 = Classification(df, 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       " 97     12.29        1.41  1.98               16.0       85.0           2.55   \n",
       " 53     13.77        1.90  2.68               17.1      115.0           3.00   \n",
       " 20     14.06        1.63  2.28               16.0      126.0           3.00   \n",
       " 30     13.73        1.50  2.70               22.5      101.0           3.00   \n",
       " 160    12.36        3.83  2.38               21.0       88.0           2.30   \n",
       " ..       ...         ...   ...                ...        ...            ...   \n",
       " 58     13.72        1.43  2.50               16.7      108.0           3.40   \n",
       " 21     12.93        3.80  2.65               18.6      102.0           2.41   \n",
       " 49     13.94        1.73  2.27               17.4      108.0           2.88   \n",
       " 64     12.17        1.45  2.53               19.0      104.0           1.89   \n",
       " 68     13.34        0.94  2.36               17.0      110.0           2.53   \n",
       " \n",
       "      flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       " 97         2.50                  0.29             1.77             2.90  1.23   \n",
       " 53         2.79                  0.39             1.68             6.30  1.13   \n",
       " 20         3.17                  0.24             2.10             5.65  1.09   \n",
       " 30         3.25                  0.29             2.38             5.70  1.19   \n",
       " 160        0.92                  0.50             1.04             7.65  0.56   \n",
       " ..          ...                   ...              ...              ...   ...   \n",
       " 58         3.67                  0.19             2.04             6.80  0.89   \n",
       " 21         2.41                  0.25             1.98             4.50  1.03   \n",
       " 49         3.54                  0.32             2.08             8.90  1.12   \n",
       " 64         1.75                  0.45             1.03             2.95  1.45   \n",
       " 68         1.30                  0.55             0.42             3.17  1.02   \n",
       " \n",
       "      od280/od315_of_diluted_wines  proline  \n",
       " 97                           2.74    428.0  \n",
       " 53                           2.93   1375.0  \n",
       " 20                           3.71    780.0  \n",
       " 30                           2.71   1285.0  \n",
       " 160                          1.58    520.0  \n",
       " ..                            ...      ...  \n",
       " 58                           2.87   1285.0  \n",
       " 21                           3.52    770.0  \n",
       " 49                           3.10   1260.0  \n",
       " 64                           2.23    355.0  \n",
       " 68                           1.93    750.0  \n",
       " \n",
       " [124 rows x 13 columns],\n",
       "      alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       " 133    12.70        3.55  2.36               21.5      106.0           1.70   \n",
       " 90     12.08        1.83  2.32               18.5       81.0           1.60   \n",
       " 17     13.83        1.57  2.62               20.0      115.0           2.95   \n",
       " 10     14.10        2.16  2.30               18.0      105.0           2.95   \n",
       " 92     12.69        1.53  2.26               20.7       80.0           1.38   \n",
       " 79     12.70        3.87  2.40               23.0      101.0           2.83   \n",
       " 154    12.58        1.29  2.10               20.0      103.0           1.48   \n",
       " 2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       " 67     12.37        1.17  1.92               19.6       78.0           2.11   \n",
       " 95     12.47        1.52  2.20               19.0      162.0           2.50   \n",
       " 62     13.67        1.25  1.92               18.0       94.0           2.10   \n",
       " 19     13.64        3.10  2.56               15.2      116.0           2.70   \n",
       " 80     12.00        0.92  2.00               19.0       86.0           2.42   \n",
       " 109    11.61        1.35  2.70               20.0       94.0           2.74   \n",
       " 0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       " 77     11.84        2.89  2.23               18.0      112.0           1.72   \n",
       " 177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       " 173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       " 38     13.07        1.50  2.10               15.5       98.0           2.40   \n",
       " 153    13.23        3.30  2.28               18.5       98.0           1.80   \n",
       " 11     14.12        1.48  2.32               16.8       95.0           2.20   \n",
       " 37     13.05        1.65  2.55               18.0       98.0           2.45   \n",
       " 1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       " 45     14.21        4.04  2.44               18.9      111.0           2.85   \n",
       " 134    12.51        1.24  2.25               17.5       85.0           2.00   \n",
       " 29     14.02        1.68  2.21               16.0       96.0           2.65   \n",
       " 170    12.20        3.03  2.32               19.0       96.0           1.25   \n",
       " 122    12.42        4.43  2.73               26.5      102.0           2.20   \n",
       " 39     14.22        3.99  2.51               13.2      128.0           3.00   \n",
       " 76     13.03        0.90  1.71               16.0       86.0           1.95   \n",
       " 55     13.56        1.73  2.46               20.5      116.0           2.96   \n",
       " 65     12.37        1.21  2.56               18.1       98.0           2.42   \n",
       " 138    13.49        3.59  2.19               19.5       88.0           1.62   \n",
       " 71     13.86        1.51  2.67               25.0       86.0           2.95   \n",
       " 57     13.29        1.97  2.68               16.8      102.0           3.00   \n",
       " 96     11.81        2.12  2.74               21.5      134.0           1.60   \n",
       " 12     13.75        1.73  2.41               16.0       89.0           2.60   \n",
       " 41     13.41        3.84  2.12               18.8       90.0           2.45   \n",
       " 22     13.71        1.86  2.36               16.6      101.0           2.61   \n",
       " 161    13.69        3.26  2.54               20.0      107.0           1.83   \n",
       " 163    12.96        3.45  2.35               18.5      106.0           1.39   \n",
       " 9      13.86        1.35  2.27               16.0       98.0           2.98   \n",
       " 15     13.63        1.81  2.70               17.2      112.0           2.85   \n",
       " 149    13.08        3.90  2.36               21.5      113.0           1.41   \n",
       " 66     13.11        1.01  1.70               15.0       78.0           2.98   \n",
       " 148    13.32        3.24  2.38               21.5       92.0           1.93   \n",
       " 137    12.53        5.51  2.64               25.0       96.0           1.79   \n",
       " 105    12.42        2.55  2.27               22.0       90.0           1.68   \n",
       " 167    12.82        3.37  2.30               19.5       88.0           1.48   \n",
       " 121    11.56        2.05  3.23               28.5      119.0           3.18   \n",
       " 141    13.36        2.56  2.35               20.0       89.0           1.40   \n",
       " 56     14.22        1.70  2.30               16.3      118.0           3.20   \n",
       " 164    13.78        2.76  2.30               22.0       90.0           1.35   \n",
       " 130    12.86        1.35  2.32               18.0      122.0           1.51   \n",
       " \n",
       "      flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       " 133        1.20                  0.17             0.84             5.00  0.78   \n",
       " 90         1.50                  0.52             1.64             2.40  1.08   \n",
       " 17         3.40                  0.40             1.72             6.60  1.13   \n",
       " 10         3.32                  0.22             2.38             5.75  1.25   \n",
       " 92         1.46                  0.58             1.62             3.05  0.96   \n",
       " 79         2.55                  0.43             1.95             2.57  1.19   \n",
       " 154        0.58                  0.53             1.40             7.60  0.58   \n",
       " 2          3.24                  0.30             2.81             5.68  1.03   \n",
       " 67         2.00                  0.27             1.04             4.68  1.12   \n",
       " 95         2.27                  0.32             3.28             2.60  1.16   \n",
       " 62         1.79                  0.32             0.73             3.80  1.23   \n",
       " 19         3.03                  0.17             1.66             5.10  0.96   \n",
       " 80         2.26                  0.30             1.43             2.50  1.38   \n",
       " 109        2.92                  0.29             2.49             2.65  0.96   \n",
       " 0          3.06                  0.28             2.29             5.64  1.04   \n",
       " 77         1.32                  0.43             0.95             2.65  0.96   \n",
       " 177        0.76                  0.56             1.35             9.20  0.61   \n",
       " 173        0.61                  0.52             1.06             7.70  0.64   \n",
       " 38         2.64                  0.28             1.37             3.70  1.18   \n",
       " 153        0.83                  0.61             1.87            10.52  0.56   \n",
       " 11         2.43                  0.26             1.57             5.00  1.17   \n",
       " 37         2.43                  0.29             1.44             4.25  1.12   \n",
       " 1          2.76                  0.26             1.28             4.38  1.05   \n",
       " 45         2.65                  0.30             1.25             5.24  0.87   \n",
       " 134        0.58                  0.60             1.25             5.45  0.75   \n",
       " 29         2.33                  0.26             1.98             4.70  1.04   \n",
       " 170        0.49                  0.40             0.73             5.50  0.66   \n",
       " 122        2.13                  0.43             1.71             2.08  0.92   \n",
       " 39         3.04                  0.20             2.08             5.10  0.89   \n",
       " 76         2.03                  0.24             1.46             4.60  1.19   \n",
       " 55         2.78                  0.20             2.45             6.25  0.98   \n",
       " 65         2.65                  0.37             2.08             4.60  1.19   \n",
       " 138        0.48                  0.58             0.88             5.70  0.81   \n",
       " 71         2.86                  0.21             1.87             3.38  1.36   \n",
       " 57         3.23                  0.31             1.66             6.00  1.07   \n",
       " 96         0.99                  0.14             1.56             2.50  0.95   \n",
       " 12         2.76                  0.29             1.81             5.60  1.15   \n",
       " 41         2.68                  0.27             1.48             4.28  0.91   \n",
       " 22         2.88                  0.27             1.69             3.80  1.11   \n",
       " 161        0.56                  0.50             0.80             5.88  0.96   \n",
       " 163        0.70                  0.40             0.94             5.28  0.68   \n",
       " 9          3.15                  0.22             1.85             7.22  1.01   \n",
       " 15         2.91                  0.30             1.46             7.30  1.28   \n",
       " 149        1.39                  0.34             1.14             9.40  0.57   \n",
       " 66         3.18                  0.26             2.28             5.30  1.12   \n",
       " 148        0.76                  0.45             1.25             8.42  0.55   \n",
       " 137        0.60                  0.63             1.10             5.00  0.82   \n",
       " 105        1.84                  0.66             1.42             2.70  0.86   \n",
       " 167        0.66                  0.40             0.97            10.26  0.72   \n",
       " 121        5.08                  0.47             1.87             6.00  0.93   \n",
       " 141        0.50                  0.37             0.64             5.60  0.70   \n",
       " 56         3.00                  0.26             2.03             6.38  0.94   \n",
       " 164        0.68                  0.41             1.03             9.58  0.70   \n",
       " 130        1.25                  0.21             0.94             4.10  0.76   \n",
       " \n",
       "      od280/od315_of_diluted_wines  proline  \n",
       " 133                          1.29    600.0  \n",
       " 90                           2.27    480.0  \n",
       " 17                           2.57   1130.0  \n",
       " 10                           3.17   1510.0  \n",
       " 92                           2.06    495.0  \n",
       " 79                           3.13    463.0  \n",
       " 154                          1.55    640.0  \n",
       " 2                            3.17   1185.0  \n",
       " 67                           3.48    510.0  \n",
       " 95                           2.63    937.0  \n",
       " 62                           2.46    630.0  \n",
       " 19                           3.36    845.0  \n",
       " 80                           3.12    278.0  \n",
       " 109                          3.26    680.0  \n",
       " 0                            3.92   1065.0  \n",
       " 77                           2.52    500.0  \n",
       " 177                          1.60    560.0  \n",
       " 173                          1.74    740.0  \n",
       " 38                           2.69   1020.0  \n",
       " 153                          1.51    675.0  \n",
       " 11                           2.82   1280.0  \n",
       " 37                           2.51   1105.0  \n",
       " 1                            3.40   1050.0  \n",
       " 45                           3.33   1080.0  \n",
       " 134                          1.51    650.0  \n",
       " 29                           3.59   1035.0  \n",
       " 170                          1.83    510.0  \n",
       " 122                          3.12    365.0  \n",
       " 39                           3.53    760.0  \n",
       " 76                           2.48    392.0  \n",
       " 55                           3.03   1120.0  \n",
       " 65                           2.30    678.0  \n",
       " 138                          1.82    580.0  \n",
       " 71                           3.16    410.0  \n",
       " 57                           2.84   1270.0  \n",
       " 96                           2.26    625.0  \n",
       " 12                           2.90   1320.0  \n",
       " 41                           3.00   1035.0  \n",
       " 22                           4.00   1035.0  \n",
       " 161                          1.82    680.0  \n",
       " 163                          1.75    675.0  \n",
       " 9                            3.55   1045.0  \n",
       " 15                           2.88   1310.0  \n",
       " 149                          1.33    550.0  \n",
       " 66                           3.18    502.0  \n",
       " 148                          1.62    650.0  \n",
       " 137                          1.69    515.0  \n",
       " 105                          3.30    315.0  \n",
       " 167                          1.75    685.0  \n",
       " 121                          3.69    465.0  \n",
       " 141                          2.47    780.0  \n",
       " 56                           3.31    970.0  \n",
       " 164                          1.68    615.0  \n",
       " 130                          1.29    630.0  ,\n",
       " 97     1\n",
       " 53     0\n",
       " 20     0\n",
       " 30     0\n",
       " 160    2\n",
       "       ..\n",
       " 58     0\n",
       " 21     0\n",
       " 49     0\n",
       " 64     1\n",
       " 68     1\n",
       " Name: target, Length: 124, dtype: int32,\n",
       " 133    2\n",
       " 90     1\n",
       " 17     0\n",
       " 10     0\n",
       " 92     1\n",
       " 79     1\n",
       " 154    2\n",
       " 2      0\n",
       " 67     1\n",
       " 95     1\n",
       " 62     1\n",
       " 19     0\n",
       " 80     1\n",
       " 109    1\n",
       " 0      0\n",
       " 77     1\n",
       " 177    2\n",
       " 173    2\n",
       " 38     0\n",
       " 153    2\n",
       " 11     0\n",
       " 37     0\n",
       " 1      0\n",
       " 45     0\n",
       " 134    2\n",
       " 29     0\n",
       " 170    2\n",
       " 122    1\n",
       " 39     0\n",
       " 76     1\n",
       " 55     0\n",
       " 65     1\n",
       " 138    2\n",
       " 71     1\n",
       " 57     0\n",
       " 96     1\n",
       " 12     0\n",
       " 41     0\n",
       " 22     0\n",
       " 161    2\n",
       " 163    2\n",
       " 9      0\n",
       " 15     0\n",
       " 149    2\n",
       " 66     1\n",
       " 148    2\n",
       " 137    2\n",
       " 105    1\n",
       " 167    2\n",
       " 121    1\n",
       " 141    2\n",
       " 56     0\n",
       " 164    2\n",
       " 130    2\n",
       " Name: target, dtype: int32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_2.split_dataframe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Models prepared. Apply them or use kfold (apply + evaluate)'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_2.prepare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Classification: using mean of 5 stratified folds --\n",
      "Starting LogisticRegression:\n",
      "- LogisticRegression done in 0.18 sec(s). Total time: 0.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Roger\\AppData\\Local\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Roger\\AppData\\Local\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Roger\\AppData\\Local\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Roger\\AppData\\Local\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Roger\\AppData\\Local\\miniconda3\\envs\\data_analytics\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "dict_round_2 = round_2.apply_and_evaluate_kfolds(multiclass_average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.926333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.931961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.932374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.937374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LogisticRegression\n",
       "accuracy             0.926333\n",
       "f1_score             0.931961\n",
       "precision            0.932374\n",
       "recall               0.937374"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_2.create_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {'models': [LogisticRegression(),\n",
       "   LogisticRegression(),\n",
       "   LogisticRegression(),\n",
       "   LogisticRegression(),\n",
       "   LogisticRegression()],\n",
       "  'metrics': {'accuracy': 0.9263333333333333,\n",
       "   'recall': 0.9373737373737374,\n",
       "   'precision': 0.9323737373737375,\n",
       "   'f1_score': 0.931960544428575},\n",
       "  'all_metrics': {'accuracy': [1.0, 0.96, 0.96, 0.92, 0.7916666666666666],\n",
       "   'recall': [1.0,\n",
       "    0.9722222222222222,\n",
       "    0.9629629629629629,\n",
       "    0.9259259259259259,\n",
       "    0.8257575757575758],\n",
       "   'precision': [1.0,\n",
       "    0.9583333333333334,\n",
       "    0.9696969696969697,\n",
       "    0.9393939393939394,\n",
       "    0.7944444444444444],\n",
       "   'f1_score': [1.0,\n",
       "    0.9632850241545894,\n",
       "    0.9645191409897292,\n",
       "    0.9249999999999999,\n",
       "    0.806998556998557]},\n",
       "  'variances': {'accuracy': 0.005173777777777779,\n",
       "   'recall': 0.0036760988107789423,\n",
       "   'precision': 0.005142801754922971,\n",
       "   'f1_score': 0.0044669200471591074}}}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_round_2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62be4b4d7aada9f05487a097e316e83dc3ceda15568e9d0ea281b513767b88d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
